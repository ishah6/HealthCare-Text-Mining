{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a114.txt</td>\n",
       "      <td>i think there are different questions for your...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>a61516.txt</td>\n",
       "      <td>It isnt your fault, youre doing nothing wrong....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>a24807.txt</td>\n",
       "      <td>If the problem with the butter is just the sod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>ans1455.txt</td>\n",
       "      <td>The previous injury that you described at the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>ans1315.txt</td>\n",
       "      <td>The burning feeling in the buttocks might be d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>a7369.txt</td>\n",
       "      <td>Restrain yourself to eat during the hours that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>a7333.txt</td>\n",
       "      <td>It means &lt;&lt; Electro Magnetic Source &gt;&gt; in elec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>a69597.txt</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>ans354.txt</td>\n",
       "      <td>Headache that present after waking up, is asso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>a24392.txt</td>\n",
       "      <td>FIRST OFF WHO ARE YOU?  YOU ARE A CHILD OF THE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>ans995.txt</td>\n",
       "      <td>Possibly you are experiencing a Hip tendinitis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>a31638.txt</td>\n",
       "      <td>shieet. why do you want to get rid of them? i ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>ans693.txt</td>\n",
       "      <td>It is difficult for me to say for sure whether...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>a24399.txt</td>\n",
       "      <td>IF I HAD TO I COULD PROBABLY BUT I HAVE NEVER ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>ans1335.txt</td>\n",
       "      <td>The data that you provided in your question is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>ans1734.txt</td>\n",
       "      <td>Whether or not you can get infected from herpe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ans1027.txt</td>\n",
       "      <td>Red blood cells carry oxygen to and remove was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>a61518.txt</td>\n",
       "      <td>HELLO DADDY!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>a24636.txt</td>\n",
       "      <td>First of all, I lived through the 60s; anyone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>ans15.txt</td>\n",
       "      <td>A J-stent is a thin stent put into the ureter ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>a24833.txt</td>\n",
       "      <td>take a shower!!!!!  Keep the ventilator out th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>ans1447.txt</td>\n",
       "      <td>The plexiform neurofibromas tend to progress, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>ans8.txt</td>\n",
       "      <td>A common cause of headaches is tension-type he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>a24733.txt</td>\n",
       "      <td>OKAY MY FIRST QUESTION WOULD HAVE TO BE HOW CA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>ans284.txt</td>\n",
       "      <td>Excess facial and body hair in women is called...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>a61554.txt</td>\n",
       "      <td>OMG!!! There are hundreds of this same questio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>ans1308.txt</td>\n",
       "      <td>The back pain that you have mentioned is most ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ans103.txt</td>\n",
       "      <td>Anemia can certainly cause rhythm disturbances...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>a69430.txt</td>\n",
       "      <td>Best of luck. By the way, is that a fractal im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>a7492.txt</td>\n",
       "      <td>drink Crisco or vegetable oil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>ans83.txt</td>\n",
       "      <td>After sleeping for several hours in a bad posi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>ans618.txt</td>\n",
       "      <td>I'm not sure I get what you mean. Usually dige...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>ans340.txt</td>\n",
       "      <td>Gonorrhea is caused by neisseria gonorrhoeae, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file                                               text  class\n",
       "17       a114.txt  i think there are different questions for your...      0\n",
       "1216   a61516.txt  It isnt your fault, youre doing nothing wrong....      0\n",
       "575    a24807.txt  If the problem with the butter is just the sod...      0\n",
       "507   ans1455.txt  The previous injury that you described at the ...      1\n",
       "352   ans1315.txt  The burning feeling in the buttocks might be d...      1\n",
       "1581    a7369.txt  Restrain yourself to eat during the hours that...      0\n",
       "1545    a7333.txt  It means << Electro Magnetic Source >> in elec...      0\n",
       "1462   a69597.txt                                                no       0\n",
       "1158   ans354.txt  Headache that present after waking up, is asso...      1\n",
       "160    a24392.txt  FIRST OFF WHO ARE YOU?  YOU ARE A CHILD OF THE...      0\n",
       "1869   ans995.txt  Possibly you are experiencing a Hip tendinitis...      1\n",
       "714    a31638.txt  shieet. why do you want to get rid of them? i ...      0\n",
       "1534   ans693.txt  It is difficult for me to say for sure whether...      1\n",
       "167    a24399.txt  IF I HAD TO I COULD PROBABLY BUT I HAVE NEVER ...      0\n",
       "374   ans1335.txt  The data that you provided in your question is...      1\n",
       "817   ans1734.txt  Whether or not you can get infected from herpe...      1\n",
       "32    ans1027.txt  Red blood cells carry oxygen to and remove was...      1\n",
       "1218   a61518.txt                                      HELLO DADDY!       0\n",
       "404    a24636.txt  First of all, I lived through the 60s; anyone ...      0\n",
       "556     ans15.txt  A J-stent is a thin stent put into the ureter ...      1\n",
       "601    a24833.txt  take a shower!!!!!  Keep the ventilator out th...      0\n",
       "498   ans1447.txt  The plexiform neurofibromas tend to progress, ...      1\n",
       "1652     ans8.txt  A common cause of headaches is tension-type he...      1\n",
       "501    a24733.txt  OKAY MY FIRST QUESTION WOULD HAVE TO BE HOW CA...      0\n",
       "1080   ans284.txt  Excess facial and body hair in women is called...      1\n",
       "1254   a61554.txt  OMG!!! There are hundreds of this same questio...      0\n",
       "344   ans1308.txt  The back pain that you have mentioned is most ...      1\n",
       "35     ans103.txt  Anemia can certainly cause rhythm disturbances...      1\n",
       "1298   a69430.txt  Best of luck. By the way, is that a fractal im...      0\n",
       "1705    a7492.txt                     drink Crisco or vegetable oil       0\n",
       "1686    ans83.txt  After sleeping for several hours in a bad posi...      1\n",
       "1451   ans618.txt  I'm not sure I get what you mean. Usually dige...      1\n",
       "1143   ans340.txt  Gonorrhea is caused by neisseria gonorrhoeae, ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) \n",
    "#Read the data into a pandas DataFrame\n",
    "import os\n",
    "def data2df (path, label):\n",
    "    file, text = [], []\n",
    "    for f in os.listdir(path):\n",
    "        file.append(f)\n",
    "        fhr = open(path+f, 'r', encoding='utf-8', errors='ignore') \n",
    "        t = fhr.read()\n",
    "        text.append(t)\n",
    "        fhr.close()\n",
    "    return(pd.DataFrame({'file': file, 'text': text, 'class':label}))\n",
    "\n",
    "df_neg = data2df('HealthProNonPro/NonPro/', 0) # NonPro\n",
    "df_pos = data2df('HealthProNonPro/Pro/', 1) # Pro\n",
    "\n",
    "df = pd.concat([df_pos, df_neg], axis=0)\n",
    "df.sample(frac=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) \n",
    "#Setup the data for Training/Testing. Use 20% for testing.\n",
    "X, y = df['text'], df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "Xtrain = Xtrain.copy()\n",
    "Xtest = Xtest.copy()\n",
    "ytrain = ytrain.copy()\n",
    "ytest = ytest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) \n",
    "#Use Spacy to preprocess the data. Explore and pick appropriate preprocessing steps.\n",
    "def custom_tokenizer(doc):\n",
    "\n",
    "    # use spacy to filter out noise\n",
    "    tokens = [token.lemma_.lower() \n",
    "                        for token in doc \n",
    "                               if (\n",
    "                                    len(token) >= 2 and # only preserve tokens that are greater than 2 characters long\n",
    "                                    #token.pos_ in ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV'] and # only preserve selected pos\n",
    "                                    #token.text in nlp.vocab and # check if token in vocab \n",
    "                                    token.is_alpha and # only preserve tokens that are fully alpha (not numeric or alpha-numeric)\n",
    "                                    #not token.is_digit and # get rid of tokens that are fully numeric\n",
    "                                    not token.is_punct and # get rid of tokens that are punctuations\n",
    "                                    not token.is_space and # get rid of tokens that are spaces\n",
    "                                    not token.is_stop and # get rid of tokens that are stop words\n",
    "                                    not token.is_currency # get rid of tokens that denote currencies\n",
    "                                )\n",
    "                   ]\n",
    "\n",
    "    # return cleaned-up text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439     common cause itchy palm contact dermatitis exp...\n",
       "720     take money support famlys need like food shelt...\n",
       "307                      not sign married doctor approval\n",
       "87      speed amphetamine psychostimulant commonly abu...\n",
       "1066                                          tantric sex\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing Xtrain using Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "corpus = nlp.pipe(list(Xtrain))\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in corpus]\n",
    "Xtrain = pd.Series(clean_corpus,index=Xtrain.index)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4)\n",
    "#Setup a Pipeline with TfidfVectorizer and Naïve Bayes. \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb=Pipeline(steps=[('tfidf',TfidfVectorizer()),('mnb',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) \n",
    "#Do Grid Search with 4-fold Cross Validation to search for the best values for the following two hyper-parameters (and any additional hyper parameters you may want to tune):\n",
    "# sublinear_tf in TfidfVectorizer \n",
    "# alpha in Naïve Bayes \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'mnb__alpha': [0.2,0.5,0.7], # getting best alpha\n",
    "    'tfidf__sublinear_tf':[True,False], # fublinear_tf from tfidf\n",
    "    'tfidf__norm':['l1','l2'] #finding best norm\n",
    "}\n",
    "gscv = GridSearchCV(nb, param_grid, cv=4, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Pipeline(memory=None,\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('mnb',\n",
      "                 MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
      "         verbose=False) \n",
      "\n",
      "--------------------------------------------------\n",
      "0.9334016393442623 \n",
      "\n",
      "--------------------------------------------------\n",
      "{'mnb__alpha': 0.2, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False} \n",
      "\n",
      "--------------------------------------------------\n",
      "{'mean_fit_time': array([0.17749393, 0.12144786, 0.14295471, 0.12771207, 0.12474322,\n",
      "       0.13932037, 0.12533188, 0.15219462, 0.17567253, 0.14452535,\n",
      "       0.1418364 , 0.14237761]), 'std_fit_time': array([0.03890834, 0.01308484, 0.0270606 , 0.02302492, 0.00198614,\n",
      "       0.01983745, 0.00827157, 0.03041628, 0.03541332, 0.01244025,\n",
      "       0.01529123, 0.0105309 ]), 'mean_score_time': array([0.04478061, 0.04262197, 0.03673428, 0.03903598, 0.03656119,\n",
      "       0.03838134, 0.03319812, 0.05452502, 0.04783452, 0.03746903,\n",
      "       0.04172599, 0.04071885]), 'std_score_time': array([0.00385531, 0.00582035, 0.00443738, 0.00516732, 0.0009122 ,\n",
      "       0.00665884, 0.00166926, 0.0208546 , 0.01071326, 0.00165243,\n",
      "       0.00678845, 0.00713996]), 'param_mnb__alpha': masked_array(data=[0.2, 0.2, 0.2, 0.2, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7,\n",
      "                   0.7],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_tfidf__norm': masked_array(data=['l1', 'l1', 'l2', 'l2', 'l1', 'l1', 'l2', 'l2', 'l1',\n",
      "                   'l1', 'l2', 'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_tfidf__sublinear_tf': masked_array(data=[True, False, True, False, True, False, True, False,\n",
      "                   True, False, True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'mnb__alpha': 0.2, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.2, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.2, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.2, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.7, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.7, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.7, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.7, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}], 'split0_test_score': array([0.92633015, 0.92769441, 0.93042292, 0.93315143, 0.92360164,\n",
      "       0.92223738, 0.92087312, 0.92496589, 0.92223738, 0.92223738,\n",
      "       0.92223738, 0.92360164]), 'split1_test_score': array([0.93989071, 0.93989071, 0.94262295, 0.94125683, 0.93989071,\n",
      "       0.93989071, 0.94262295, 0.94672131, 0.93852459, 0.93989071,\n",
      "       0.93989071, 0.94262295]), 'split2_test_score': array([0.90710383, 0.9057377 , 0.91530055, 0.91666667, 0.90027322,\n",
      "       0.90300546, 0.91530055, 0.91530055, 0.89754098, 0.8989071 ,\n",
      "       0.91120219, 0.91530055]), 'split3_test_score': array([0.92749658, 0.92749658, 0.94117647, 0.94254446, 0.92202462,\n",
      "       0.92339261, 0.93570451, 0.93433653, 0.92339261, 0.92339261,\n",
      "       0.93160055, 0.93433653]), 'mean_test_score': array([0.92520492, 0.92520492, 0.93237705, 0.93340164, 0.92144809,\n",
      "       0.92213115, 0.92862022, 0.93032787, 0.9204235 , 0.92110656,\n",
      "       0.92622951, 0.92896175]), 'std_test_score': array([0.01172427, 0.0123095 , 0.01092856, 0.01031144, 0.0140844 ,\n",
      "       0.01306517, 0.01099669, 0.01161252, 0.01469147, 0.01459589,\n",
      "       0.01069131, 0.01037877]), 'rank_test_score': array([ 7,  7,  2,  1, 10,  9,  5,  3, 12, 11,  6,  4])} \n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 6) \n",
    "#Use the Best Estimator resulting from the Grid Search for Prediction/Evaluation. Print the following evaluation metrics:\n",
    "# Accuracy score\n",
    "# Confusion matrix\n",
    "# Classification report\n",
    "\n",
    "gscv.fit(Xtrain, ytrain)\n",
    "\n",
    "print (\"-\"*50)\n",
    "print(gscv.best_estimator_, \"\\n\")\n",
    "print (\"-\"*50)\n",
    "print(gscv.best_score_, \"\\n\")\n",
    "print (\"-\"*50)\n",
    "print(gscv.best_params_, \"\\n\")\n",
    "print (\"-\"*50)\n",
    "print(gscv.cv_results_, \"\\n\")\n",
    "print (\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956     procrastinating inevitable drinking problem da...\n",
       "1247    understand concern pain knee surgery pain weig...\n",
       "102     swallow complex act involve mouth throat area ...\n",
       "813        warning eat ask long eat find well way protest\n",
       "994                                          course break\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Xtest using Spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "corpus = nlp.pipe(list(Xtest))\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in corpus]\n",
    "Xtest = pd.Series(clean_corpus,index=Xtest.index)\n",
    "Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9427012278308322\n",
      "[[322  36]\n",
      " [  6 369]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       358\n",
      "           1       0.91      0.98      0.95       375\n",
      "\n",
      "    accuracy                           0.94       733\n",
      "   macro avg       0.95      0.94      0.94       733\n",
      "weighted avg       0.95      0.94      0.94       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict/Evaluate Best Estimator on Xtest\n",
    "\n",
    "ypred = gscv.best_estimator_.predict(Xtest)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 369 \tFP: 36 \tTN: 322 \tTP: 369\n"
     ]
    }
   ],
   "source": [
    "# 7) \n",
    "#Extract the true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP)\n",
    "\n",
    "TN, FP, FN, TP = metrics.confusion_matrix(y_true=ytest, y_pred=ypred).ravel()\n",
    "print(\"TP:\",TP,\"\\t\"\n",
    "      'FP:',FP,\"\\t\" \n",
    "      'TN:',TN,\"\\t\" \n",
    "      'TP:',TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9427012278308322\n"
     ]
    }
   ],
   "source": [
    "#Overall_Accuracy\n",
    "\n",
    "Overall_Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Overall Accuracy:\",Overall_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Class 0: 0.9817073170731707\n",
      "Precision for Class 1: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "#Precision for Class 0 and Class 1\n",
    "\n",
    "Pre_Cl_0 = TN / (TN + FN)\n",
    "Pre_Cl_1 = TP / (TP + FP)\n",
    "\n",
    "print(\"Precision for Class 0:\",Pre_Cl_0)\n",
    "print(\"Precision for Class 1:\",Pre_Cl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Class 0: 0.8994413407821229\n",
      "Recall for Class 1: 0.984\n"
     ]
    }
   ],
   "source": [
    "#Recall for Class 0 and Class 1\n",
    "\n",
    "Rec_Cl_0 = TN / (TN + FP)\n",
    "Rec_Cl_1 = TP / (TP + FN)\n",
    "\n",
    "print(\"Recall for Class 0:\",Rec_Cl_0)\n",
    "print(\"Recall for Class 1:\",Rec_Cl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for Class 0: 0.9387755102040817\n",
      "F1-score for Class 1: 0.9461538461538462\n"
     ]
    }
   ],
   "source": [
    "#F1-Score for Class 0 and Class 1\n",
    "\n",
    "F1scr_Cl_0 = (2*Rec_Cl_0*Pre_Cl_0)/(Rec_Cl_0+Pre_Cl_0)\n",
    "F1scr_Cl_1 = (2*Rec_Cl_1*Pre_Cl_1)/(Rec_Cl_1+Pre_Cl_1)\n",
    "\n",
    "print(\"F1-score for Class 0:\",F1scr_Cl_0)\n",
    "print(\"F1-score for Class 1:\",F1scr_Cl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
